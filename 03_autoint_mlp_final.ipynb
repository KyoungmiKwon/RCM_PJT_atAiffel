{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 20:49:28.711665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, MaxPooling2D, Conv2D, Dropout, Lambda, Dense, Flatten, Activation, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_normal, Zeros, TruncatedNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 15)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "# csv 데이터이므로 read_csv로 가져옵니다.\n",
    "data_path = './data/'\n",
    "movielens_rcmm = pd.read_csv(f\"{data_path}/movielens_rcmm_v2.csv\", dtype=str)\n",
    "print(movielens_rcmm.shape)\n",
    "movielens_rcmm.head()\n",
    "\n",
    "# 2. 라벨 인코더(label encoder)\n",
    "# sklearn의 LabelEncoder(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "# label은 제외한 각 컬럼을 돌면서 각각의 고윳값들을 0부터 n까지 매핑시킵니다.\n",
    "label_encoders = {col: LabelEncoder() for col in movielens_rcmm.columns[:-1]} # label은 제외\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    movielens_rcmm[col] = le.fit_transform(movielens_rcmm[col])\n",
    "\n",
    "movielens_rcmm['label'] = movielens_rcmm['label'].astype(np.float32)\n",
    "\n",
    "# 3. 학습 데이터와 테스트데이터로 분리, 0.2 정도로 분리\n",
    "train_df, test_df = train_test_split(movielens_rcmm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_decade</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>rating_month</th>\n",
       "      <th>rating_decade</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3374</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3615</td>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2503</td>\n",
       "      <td>9</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1374</td>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  movie_decade  movie_year  rating_year  rating_month  \\\n",
       "0        0       189             6          55            1             0   \n",
       "1        0      3374             8          76            1             0   \n",
       "2        0      3615             5          44            1             0   \n",
       "3        0      2503             9          80            1             0   \n",
       "4        0      1374             8          78            1             0   \n",
       "\n",
       "   rating_decade  genre1  genre2  genre3  gender  age  occupation   zip  label  \n",
       "0              0       7      17      15       0    0           2  1588    1.0  \n",
       "1              0       2       2       8       0    0           2  1588    0.0  \n",
       "2              0      11      12      15       0    0           2  1588    0.0  \n",
       "3              0       7      17      15       0    0           2  1588    1.0  \n",
       "4              0       2       2       2       0    0           2  1588    1.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens_rcmm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`레이어 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 레이어\n",
    "\n",
    "class FeaturesEmbedding(Layer):  \n",
    "    '''\n",
    "    임베딩 레이어입니다. \n",
    "    - 만약 피처(feature) 3개가 각각 10개, 20개, 30개의 고유값을 가진다면 feature_dims는 [10, 20, 30] 형태를 띄게 됩니다.\n",
    "    - 전체 임베딩을 해야 할 개수는 10+20+30 = 60이므로 '60 x 임베딩_차원_크기'의 행렬이 생성되게 됩니다.\n",
    "    '''\n",
    "    def __init__(self, field_dims, embed_dim, **kwargs):\n",
    "        super(FeaturesEmbedding, self).__init__(**kwargs)\n",
    "        self.total_dim = sum(field_dims)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.total_dim, output_dim=self.embed_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 임베딩을 빌드하고 초기화합니다.\n",
    "        self.embedding.build(input_shape)\n",
    "        self.embedding.set_weights([tf.keras.initializers.GlorotUniform()(shape=self.embedding.weights[0].shape)])\n",
    "\n",
    "    def call(self, x):\n",
    "        # 들어온 입력의 임베딩을 가져니다.\n",
    "        x = x + tf.constant(self.offsets)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 퍼셉트론 레이어\n",
    "\n",
    "class MultiLayerPerceptron(Layer):  \n",
    "    '''\n",
    "    DNN 레이어입니다.\n",
    "    - Tensorflow Keras에서는 Dense 레이어를 쌓아올린 구조입니다.\n",
    "    - 필요에 따라 배치 정규화도 사용할 수 있습니다.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, init_std=0.0001, output_layer=True):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        hidden_units = [input_dim] + list(hidden_units)\n",
    "        if output_layer:\n",
    "            hidden_units += [1]\n",
    "        # Dense layer를 쌓아올립니다.\n",
    "        self.linears = [Dense(units, activation=None, kernel_initializer=tf.random_normal_initializer(stddev=init_std),\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)) for units in hidden_units[1:]]\n",
    "        # 활성화 함수를 세팅합니다.\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        # 필요하다면 배치정규화도 진행합니다.\n",
    "        if self.use_bn:\n",
    "            self.bn = [BatchNormalization() for _ in hidden_units[1:]]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for i in range(len(self.linears)):\n",
    "            # input data가 들어오면 layer를 돌면서 벡터 값을 가져오게 됩니다.\n",
    "            x = self.linears[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn[i](x, training=training)\n",
    "            # 각 layer마다 나온 벡터 값에 활성화 함수와 dropout을 적용시켜 비선형성 구조와 과적합을 방지합니다.\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer):  \n",
    "    '''\n",
    "    멀티 헤드 셀프 어텐션 레이어입니다.\n",
    "    - 위에 작성한 수식과 같이 동작됩니다.\n",
    "    - 필요에 따라 잔차 연결(residual connection)도 진행합니다.\n",
    "    '''\n",
    "    def __init__(self, att_embedding_size=8, head_num=2, use_res=True, scaling=False, seed=1024, **kwargs):\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        self.att_embedding_size = att_embedding_size\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.seed = seed\n",
    "        self.scaling = scaling\n",
    "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(input_shape)))\n",
    "        embedding_size = int(input_shape[-1])\n",
    "        # 쿼리에 해당하는 매트릭스입니다. \n",
    "        self.W_Query = self.add_weight(name='query', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed))\n",
    "        # 키에 해당되는 매트릭스입니다.\n",
    "        self.W_key = self.add_weight(name='key', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=TruncatedNormal(seed=self.seed + 1))\n",
    "        # 값(value)에 해당되는 매트릭스입니다.\n",
    "        self.W_Value = self.add_weight(name='value', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed + 2))\n",
    "        # 필요하다면 잔차 연결도 할 수 있습니다.\n",
    "        if self.use_res:\n",
    "            self.W_Res = self.add_weight(name='res', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=TruncatedNormal(seed=self.seed))\n",
    "\n",
    "        super(MultiHeadSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if K.ndim(inputs) != 3:\n",
    "            raise ValueError(\"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (K.ndim(inputs)))\n",
    "        \n",
    "        # 입력이 들어오면 쿼리, 키, 값(value)에 매칭되어 각각의 값을 가지고 옵니다.\n",
    "        querys = tf.tensordot(inputs, self.W_Query, axes=(-1, 0))  \n",
    "        keys = tf.tensordot(inputs, self.W_key, axes=(-1, 0))\n",
    "        values = tf.tensordot(inputs, self.W_Value, axes=(-1, 0))\n",
    "\n",
    "        # 헤드 개수에 따라 데이터를 분리해줍니다.\n",
    "        querys = tf.stack(tf.split(querys, self.head_num, axis=2))\n",
    "        keys = tf.stack(tf.split(keys, self.head_num, axis=2))\n",
    "        values = tf.stack(tf.split(values, self.head_num, axis=2))\n",
    "        \n",
    "        # 쿼리와 키를 먼저 곱해줍니다. 위 이미지의 식 (5)와 같습니다.\n",
    "        inner_product = tf.matmul(querys, keys, transpose_b=True)\n",
    "        if self.scaling:\n",
    "            inner_product /= self.att_embedding_size ** 0.5\n",
    "        self.normalized_att_scores =  tf.nn.softmax(inner_product)\n",
    "        \n",
    "        # 쿼리와 키에서 나온 어텐션 값을 값(value)에 곱해줍니다. 식 (6)과 같습니다.\n",
    "        result = tf.matmul(self.normalized_att_scores, values)\n",
    "        # 식 (7)과 같이 쪼개어진 멀테 헤드를 모아줍니다.\n",
    "        result = tf.concat(tf.split(result, self.head_num, ), axis=-1)\n",
    "        result = tf.squeeze(result, axis=0) \n",
    "\n",
    "        if self.use_res:\n",
    "            result += tf.tensordot(inputs, self.W_Res, axes=(-1, 0))\n",
    "        result = tf.nn.relu(result)\n",
    "        \n",
    "        # 그 결과 값을 리턴합니다.\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (None, input_shape[1], self.att_embedding_size * self.head_num)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res,'seed': self.seed}\n",
    "        base_config = super(MultiHeadSelfAttention, self).get_config()\n",
    "        base_config.update(config)\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autoint model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 코드 \n",
    "# class AutoIntMLP(nn.Module):\n",
    "\n",
    "#     def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "#                  l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, dnn_dropout=0.4, init_std=0.0001, device='cpu'):\n",
    "\n",
    "#         super(AutoIntMLP, self).__init__()\n",
    "#         self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "#         self.num_fields = len(field_dims)\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.att_output_dim = self.num_fields * self.embedding_size\n",
    "#         self.embed_output_dim = len(field_dims) * embed_dim\n",
    "\n",
    "#         self.dnn_linear = nn.Linear(self.att_output_dim, 1, bias=False).to(device)\n",
    "#         self.dnn_hidden_units = dnn_hidden_units\n",
    "#         self.att_layer_num = att_layer_num\n",
    "#         self.dnn = MultiLayerPerceptron(self.embed_output_dim, dnn_hidden_units,\n",
    "#                            activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "#                            init_std=init_std, output_layer=True, device=device)\n",
    "#         self.int_layers = nn.ModuleList(\n",
    "#             [MultiHeadSelfAttention(self.embedding_size, att_head_num, att_res, device=device) for _ in range(att_layer_num)])\n",
    "\n",
    "#         self.to(device)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         embed_x = self.embedding(X)\n",
    "#         dnn_embed = embed_x\n",
    "#         att_input = embed_x\n",
    "        \n",
    "#         for cnt, layer in enumerate(self.int_layers):\n",
    "#             att_input = layer(att_input)\n",
    "            \n",
    "#         att_output = torch.flatten(att_input, start_dim=1)\n",
    "        \n",
    "#         att_output = F.relu(self.dnn_linear(att_output))\n",
    "#         # autoint MLP\n",
    "#         dnn_output = self.dnn(dnn_embed.view(-1, self.embed_output_dim))\n",
    "        \n",
    "#         y_pred = torch.sigmoid(att_output + dnn_output)\n",
    "\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내가 만든 코드 with GPT\n",
    "\n",
    "\n",
    "# class AutoIntMLP(Layer): \n",
    "#     def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "#                  l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, dnn_dropout=0.4, init_std=0.0001):\n",
    "#         super(AutoIntMLP, self).__init__()\n",
    "#         self.embedding = FeaturesEmbedding(field_dims, embedding_size)\n",
    "#         self.num_fields = len(field_dims)\n",
    "#         self.embedding_size = embedding_size\n",
    "\n",
    "#         self.final_layer = Dense(1, use_bias=False, kernel_initializer=tf.random_normal_initializer(stddev=init_std))\n",
    "        \n",
    "#         # self.dnn = # [[YOUR CODE]]\n",
    "#         self.dnn = MultiLayerPerceptron(\n",
    "#             input_dim=self.num_fields * embedding_size,\n",
    "#             hidden_units=dnn_hidden_units,\n",
    "#             activation=dnn_activation,\n",
    "#             dropout_rate=dnn_dropout,\n",
    "#             use_bn=dnn_use_bn,\n",
    "#             l2_reg=l2_reg_dnn,\n",
    "#             init_std=init_std,\n",
    "#         #    name=\"dnn_mlp\"\n",
    "#         )\n",
    "\n",
    "#         # self.int_layers = # [[YOUR CODE]]\n",
    "#         self.int_layers = [\n",
    "#             MultiHeadSelfAttention(\n",
    "#                 att_embedding_size=embedding_size,\n",
    "#                 head_num=att_head_num,\n",
    "#                 use_res=att_res,\n",
    "#                 name=f\"att_layer_{i}\"\n",
    "#             )\n",
    "#             for i in range(att_layer_num)\n",
    "#         ]\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         embed_x = self.embedding(inputs)\n",
    "#         dnn_embed = tf.reshape(embed_x, shape=(-1, self.embedding_size * self.num_fields))\n",
    "\n",
    "#         att_input = embed_x\n",
    "#         for layer in self.int_layers:\n",
    "#             att_input = layer(att_input)\n",
    "\n",
    "#         att_output = Flatten()(att_input)\n",
    "#         att_output = self.final_layer(att_output)\n",
    "        \n",
    "#         # dnn_output = # [[YOUR CODE]]\n",
    "#         dnn_output = self.dnn(dnn_embed) \n",
    "\n",
    "#         # y_pred = # [[YOUR CODE]]\n",
    "#         y_pred = tf.nn.sigmoid(att_output + dnn_output)  \n",
    "        \n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형철 퍼실님 코드\n",
    "\n",
    "class AutoIntMLP(Layer): \n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, dnn_dropout=0.4, init_std=0.0001):\n",
    "        super(AutoIntMLP, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embedding_size)\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.final_layer = Dense(1, use_bias=False, kernel_initializer=tf.random_normal_initializer(stddev=init_std))\n",
    "        \n",
    "        self.dnn = tf.keras.Sequential()\n",
    "        for units in dnn_hidden_units:\n",
    "            self.dnn.add(Dense(units, activation=dnn_activation,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l2_reg_dnn),\n",
    "                               kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "            if dnn_use_bn:\n",
    "                self.dnn.add(BatchNormalization())\n",
    "            self.dnn.add(Activation(dnn_activation))\n",
    "            if dnn_dropout > 0:\n",
    "                self.dnn.add(Dropout(dnn_dropout))\n",
    "        self.dnn.add(Dense(1, kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "\n",
    "        self.int_layers = [MultiHeadSelfAttention(att_embedding_size=embedding_size, head_num=att_head_num, use_res=att_res) for _ in range(att_layer_num)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embed_x = self.embedding(inputs)\n",
    "        dnn_embed = tf.reshape(embed_x, shape=(-1, self.embedding_size * self.num_fields))\n",
    "\n",
    "        att_input = embed_x\n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "\n",
    "        att_output = Flatten()(att_input)\n",
    "        att_output = self.final_layer(att_output)\n",
    "        \n",
    "        dnn_output = self.dnn(dnn_embed)\n",
    "        y_pred = tf.keras.activations.sigmoid(att_output + dnn_output)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AutoInt 레이어를 가지고 있는 모델 본체입니다. 해당 모델을 활용해 훈련을 진행합니다.\n",
    "# class AutoIntModel(Model):\n",
    "#     def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, \n",
    "#                  att_res=True, \n",
    "#                  l2_reg_dnn=0, \n",
    "#                  l2_reg_embedding=1e-5, \n",
    "#                  dnn_use_bn=False, \n",
    "#                  dnn_dropout=0, \n",
    "#                  init_std=0.0001):\n",
    "#         super(AutoIntMLPModel, self).__init__()\n",
    "#         self.autoInt_layer = AutoIntMLP(\n",
    "#             field_dims, \n",
    "#             embedding_size, \n",
    "#             att_layer_num=att_layer_num, \n",
    "#             att_head_num=att_head_num,\n",
    "#             att_res=att_res, \n",
    "#             l2_reg_dnn=l2_reg_dnn, \n",
    "#             dnn_dropout=dnn_dropout, \n",
    "#             init_std=init_std\n",
    "#             )\n",
    "\n",
    "#     def call(self, inputs, training=False):\n",
    "#         return self.autoInt_layer(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoInt 레이어를 가지고 있는 모델 본체입니다. 해당 모델을 활용해 훈련을 진행합니다.\n",
    "\n",
    "class AutoIntMLPModel(Model):\n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2,\n",
    "                 att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False,\n",
    "                 dnn_dropout=0.4, init_std=0.0001):\n",
    "        super(AutoIntMLPModel, self).__init__()\n",
    "        self.autoInt_layer = AutoIntMLP(\n",
    "            field_dims=field_dims,\n",
    "            embedding_size=embedding_size,\n",
    "            att_layer_num=att_layer_num,\n",
    "            att_head_num=att_head_num,\n",
    "            att_res=att_res,\n",
    "            dnn_hidden_units=dnn_hidden_units,\n",
    "            dnn_activation=dnn_activation,\n",
    "            l2_reg_dnn=l2_reg_dnn,\n",
    "            l2_reg_embedding=l2_reg_embedding,\n",
    "            dnn_use_bn=dnn_use_bn,\n",
    "            dnn_dropout=dnn_dropout,\n",
    "            init_std=init_std\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.autoInt_layer(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 컬럼들과 레이블 정의\n",
    "# 필드의 각 고유 개수를 정의하는 field_dims를 정의합니다. 이는  임베딩 때 활용됩니다. \n",
    "u_i_feature = ['user_id', 'movie_id']\n",
    "meta_features = ['movie_decade', 'movie_year', 'rating_year', 'rating_month', 'rating_decade', 'genre1','genre2', 'genre3', 'gender', 'age', 'occupation', 'zip']\n",
    "label = 'label'\n",
    "field_dims = np.max(movielens_rcmm[u_i_feature + meta_features].astype(np.int64).values, axis=0) + 1\n",
    "field_dims\n",
    "\n",
    "# 에포크, 학습률, 드롭아웃, 배치사이즈, 임베딩 크기 등 정의\n",
    "epochs=5\n",
    "learning_rate= 0.0001\n",
    "dropout= 0.4\n",
    "batch_size = 2048\n",
    "embed_dim= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "autoInt_model = AutoIntMLPModel(field_dims, embed_dim, att_layer_num=3, att_head_num=2, att_res=True,\n",
    "                             l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, \n",
    "                              dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                             dnn_dropout=dropout, init_std=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoInt_model = AutoIntMLPModel(field_dims, embed_dim, att_layer_num=3, att_head_num=2, att_res=True,\n",
    "                             l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False\n",
    "                             , dnn_dropout=dropout, init_std=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_int_mlp_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " auto_int_mlp_2 (AutoIntMLP  multiple                  232977    \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232977 (910.07 KB)\n",
      "Trainable params: 232977 (910.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dummy_input = tf.constant([[0] * len(field_dims)], dtype=tf.int64)\n",
    "autoInt_model(dummy_input)  # 모델을 한 번 호출하여 빌드\n",
    "autoInt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저, 오차함수 정의\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)\n",
    "autoInt_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "352/352 [==============================] - 55s 143ms/step - loss: 0.6740 - binary_crossentropy: 0.6740 - val_loss: 0.6297 - val_binary_crossentropy: 0.6297\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 58s 166ms/step - loss: 0.6033 - binary_crossentropy: 0.6033 - val_loss: 0.5880 - val_binary_crossentropy: 0.5880\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 50s 141ms/step - loss: 0.5726 - binary_crossentropy: 0.5726 - val_loss: 0.5564 - val_binary_crossentropy: 0.5564\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 48s 137ms/step - loss: 0.5431 - binary_crossentropy: 0.5431 - val_loss: 0.5433 - val_binary_crossentropy: 0.5433\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 49s 139ms/step - loss: 0.5357 - binary_crossentropy: 0.5357 - val_loss: 0.5405 - val_binary_crossentropy: 0.5405\n"
     ]
    }
   ],
   "source": [
    "history = autoInt_model.fit(train_df[u_i_feature + meta_features], train_df[label], epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_df):\n",
    "    '''모델 테스트'''\n",
    "    user_pred_info = defaultdict(list)\n",
    "    total_rows = len(test_df)\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        features = test_df.iloc[i:i + batch_size, :-1].values\n",
    "        y_pred = model.predict(features, verbose=False)\n",
    "        for feature, p in zip(features, y_pred):\n",
    "            u_i = feature[:2]\n",
    "            user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
    "    return user_pred_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/2n0cthkn35z3g9p87_tw_5wm0000gn/T/ipykernel_27769/2567990266.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
      "100%|██████████| 6038/6038 [00:00<00:00, 48564.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# 사용자에게 예측된 정보를 저장하는 딕셔너리 \n",
    "user_pred_info = {}\n",
    "# top10개\n",
    "top = 10\n",
    "# 테스트 값을 가지고 옵니다. \n",
    "mymodel_user_pred_info = test_model(autoInt_model, test_df)\n",
    "# 사용자마다 돌면서 예측 데이터 중 가장 높은 top 10만 가져옵니다. \n",
    "for user, data_info in tqdm(mymodel_user_pred_info.items(), total=len(mymodel_user_pred_info), position=0, leave=True):\n",
    "    ranklist = sorted(data_info, key=lambda s : s[1], reverse=True)[:top]\n",
    "    ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "    user_pred_info[str(user)] = ranklist\n",
    "# 원본 테스트 데이터에서 label이 1인 사용자 별 영화 정보를 가져옵니다.\n",
    "test_data = test_df[test_df['label']==1].groupby('user_id')['movie_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수는 아래의 링크에서 가져왔습니다.\n",
    "# https://www.programcreek.com/python/?code=MaurizioFD%2FRecSys2019_DeepLearning_Evaluation%2FRecSys2019_DeepLearning_Evaluation-master%2FConferences%2FKDD%2FMCRec_our_interface%2FMCRecRecommenderWrapper.py\n",
    "def get_DCG(ranklist, y_true):\n",
    "    dcg = 0.0\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item in y_true:\n",
    "            dcg += 1.0 / math.log(i + 2)\n",
    "    return  dcg\n",
    "\n",
    "def get_IDCG(ranklist, y_true):\n",
    "    idcg = 0.0\n",
    "    i = 0\n",
    "    for item in y_true:\n",
    "        if item in ranklist:\n",
    "            idcg += 1.0 / math.log(i + 2)\n",
    "            i += 1\n",
    "    return idcg\n",
    "\n",
    "def get_NDCG(ranklist, y_true):\n",
    "    '''NDCG 평가 지표'''\n",
    "    ranklist = np.array(ranklist).astype(int)\n",
    "    y_true = np.array(y_true).astype(int)\n",
    "    dcg = get_DCG(ranklist, y_true)\n",
    "    idcg = get_IDCG(y_true, y_true)\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    return round( (dcg / idcg), 5)\n",
    "\n",
    "def get_hit_rate(ranklist, y_true):\n",
    "    '''hitrate 평가 지표'''\n",
    "    c = 0\n",
    "    for y in y_true:\n",
    "        if y in ranklist:\n",
    "            c += 1\n",
    "    return round( c / len(y_true), 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5994/5994 [00:01<00:00, 5320.07it/s]\n",
      "100%|██████████| 5994/5994 [00:00<00:00, 41800.91it/s]\n"
     ]
    }
   ],
   "source": [
    "mymodel_ndcg_result = {}\n",
    "mymodel_hitrate_result = {}\n",
    "\n",
    "# 모델 예측값과 원본 테스트 데이터를 비교해서 어느정도 성능이 나왔는지 NDCG와 Hitrate를 비교합니다.\n",
    "\n",
    "# NDCG\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # NDCG 값 구하기\n",
    "    user_ndcg = get_NDCG(mymodel_pred, testset)\n",
    "\n",
    "    mymodel_ndcg_result[user] = user_ndcg\n",
    "\n",
    "# Hitrate\n",
    "for user, data_info in tqdm(test_data.items(), total=len(test_data), position=0, leave=True):\n",
    "    mymodel_pred = user_pred_info.get(str(user))\n",
    "\n",
    "    testset = list(set(np.array(data_info).astype(int)))\n",
    "    mymodel_pred = mymodel_pred[:top]\n",
    "\n",
    "    # hitrate 값 구하기\n",
    "    user_hitrate = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    # 사용자 hitrate 결과 저장\n",
    "    mymodel_hitrate_result[user] = user_hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mymodel ndcg :  0.66202\n",
      " mymodel hitrate :  0.63016\n"
     ]
    }
   ],
   "source": [
    "print(\" mymodel ndcg : \", round(np.mean(list(mymodel_ndcg_result.values())), 5))\n",
    "print(\" mymodel hitrate : \", round(np.mean(list(mymodel_hitrate_result.values())), 5))\n",
    "\n",
    "# mymodel ndcg :  0.66162\n",
    "# mymodel hitrate :  0.63048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./autoint/data/field_dims_mlp.npy', field_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwonkyoungmi/workspaces/workspace_Python/PJT_RC/.venv/lib/python3.11/site-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'tensorflow.python.ops.init_ops_v2.RandomNormal'>, which may lead to improper serialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "autoInt_model.save_weights('./autoint/model/autoIntMLP_model_weights.weights.h5')\n",
    "#'autoIntMLP_model_weights.weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./autoint/data/autoIntMLP_label_encoders.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib \n",
    "\n",
    "# 모델 객체를 pickled binary file 형태로 저장\n",
    "joblib.dump(label_encoders, './autoint/data/autoIntMLP_label_encoders.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers/auto_int_mlp/dnn/layers/dense/vars/0 (224, 32)\n",
      "layers/auto_int_mlp/dnn/layers/dense/vars/1 (32,)\n",
      "layers/auto_int_mlp/dnn/layers/dense_1/vars/0 (32, 32)\n",
      "layers/auto_int_mlp/dnn/layers/dense_1/vars/1 (32,)\n",
      "layers/auto_int_mlp/dnn/layers/dense_2/vars/0 (32, 1)\n",
      "layers/auto_int_mlp/dnn/layers/dense_2/vars/1 (1,)\n",
      "layers/auto_int_mlp/embedding/embedding/vars/0 (13375, 16)\n",
      "layers/auto_int_mlp/final_layer/vars/0 (448, 1)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention/vars/0 (16, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention/vars/1 (16, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention/vars/2 (16, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention/vars/3 (16, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_1/vars/0 (32, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_1/vars/1 (32, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_1/vars/2 (32, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_1/vars/3 (32, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_2/vars/0 (32, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_2/vars/1 (32, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_2/vars/2 (32, 32)\n",
      "layers/auto_int_mlp/int_layers/multi_head_self_attention_2/vars/3 (32, 32)\n",
      "metrics/mean/vars/0 ()\n",
      "metrics/mean/vars/1 ()\n",
      "metrics/mean_metric_wrapper/vars/0 ()\n",
      "metrics/mean_metric_wrapper/vars/1 ()\n",
      "optimizer/vars/0 ()\n",
      "optimizer/vars/1 (13375, 16)\n",
      "optimizer/vars/10 (32, 32)\n",
      "optimizer/vars/11 (32,)\n",
      "optimizer/vars/12 (32,)\n",
      "optimizer/vars/13 (32, 1)\n",
      "optimizer/vars/14 (32, 1)\n",
      "optimizer/vars/15 (1,)\n",
      "optimizer/vars/16 (1,)\n",
      "optimizer/vars/17 (16, 32)\n",
      "optimizer/vars/18 (16, 32)\n",
      "optimizer/vars/19 (16, 32)\n",
      "optimizer/vars/2 (13375, 16)\n",
      "optimizer/vars/20 (16, 32)\n",
      "optimizer/vars/21 (16, 32)\n",
      "optimizer/vars/22 (16, 32)\n",
      "optimizer/vars/23 (16, 32)\n",
      "optimizer/vars/24 (16, 32)\n",
      "optimizer/vars/25 (32, 32)\n",
      "optimizer/vars/26 (32, 32)\n",
      "optimizer/vars/27 (32, 32)\n",
      "optimizer/vars/28 (32, 32)\n",
      "optimizer/vars/29 (32, 32)\n",
      "optimizer/vars/3 (448, 1)\n",
      "optimizer/vars/30 (32, 32)\n",
      "optimizer/vars/31 (32, 32)\n",
      "optimizer/vars/32 (32, 32)\n",
      "optimizer/vars/33 (32, 32)\n",
      "optimizer/vars/34 (32, 32)\n",
      "optimizer/vars/35 (32, 32)\n",
      "optimizer/vars/36 (32, 32)\n",
      "optimizer/vars/37 (32, 32)\n",
      "optimizer/vars/38 (32, 32)\n",
      "optimizer/vars/39 (32, 32)\n",
      "optimizer/vars/4 (448, 1)\n",
      "optimizer/vars/40 (32, 32)\n",
      "optimizer/vars/5 (224, 32)\n",
      "optimizer/vars/6 (224, 32)\n",
      "optimizer/vars/7 (32,)\n",
      "optimizer/vars/8 (32,)\n",
      "optimizer/vars/9 (32, 32)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "model_path = './autoint/model'\n",
    "with h5py.File(f'{model_path}/autoIntMLP_model_weights.weights.h5', 'r') as f:\n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(name, obj.shape)\n",
    "    f.visititems(print_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoInt_layer/dnn/layers/dense/vars/0 (224, 32)\n",
      "autoInt_layer/dnn/layers/dense/vars/1 (32,)\n",
      "autoInt_layer/dnn/layers/dense_1/vars/0 (32, 32)\n",
      "autoInt_layer/dnn/layers/dense_1/vars/1 (32,)\n",
      "autoInt_layer/dnn/layers/dense_2/vars/0 (32, 1)\n",
      "autoInt_layer/dnn/layers/dense_2/vars/1 (1,)\n",
      "autoInt_layer/embedding/embedding/vars/0 (13375, 16)\n",
      "autoInt_layer/final_layer/vars/0 (448, 1)\n",
      "autoInt_layer/int_layers/multi_head_self_attention/vars/0 (16, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention/vars/1 (16, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention/vars/2 (16, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention/vars/3 (16, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_1/vars/0 (32, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_1/vars/1 (32, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_1/vars/2 (32, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_1/vars/3 (32, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_2/vars/0 (32, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_2/vars/1 (32, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_2/vars/2 (32, 32)\n",
      "autoInt_layer/int_layers/multi_head_self_attention_2/vars/3 (32, 32)\n",
      "optimizer/vars/0 ()\n",
      "optimizer/vars/1 ()\n",
      "optimizer/vars/10 (32, 32)\n",
      "optimizer/vars/11 (32, 32)\n",
      "optimizer/vars/12 (32,)\n",
      "optimizer/vars/13 (32,)\n",
      "optimizer/vars/14 (32, 1)\n",
      "optimizer/vars/15 (32, 1)\n",
      "optimizer/vars/16 (1,)\n",
      "optimizer/vars/17 (1,)\n",
      "optimizer/vars/18 (16, 32)\n",
      "optimizer/vars/19 (16, 32)\n",
      "optimizer/vars/2 (13375, 16)\n",
      "optimizer/vars/20 (16, 32)\n",
      "optimizer/vars/21 (16, 32)\n",
      "optimizer/vars/22 (16, 32)\n",
      "optimizer/vars/23 (16, 32)\n",
      "optimizer/vars/24 (16, 32)\n",
      "optimizer/vars/25 (16, 32)\n",
      "optimizer/vars/26 (32, 32)\n",
      "optimizer/vars/27 (32, 32)\n",
      "optimizer/vars/28 (32, 32)\n",
      "optimizer/vars/29 (32, 32)\n",
      "optimizer/vars/3 (13375, 16)\n",
      "optimizer/vars/30 (32, 32)\n",
      "optimizer/vars/31 (32, 32)\n",
      "optimizer/vars/32 (32, 32)\n",
      "optimizer/vars/33 (32, 32)\n",
      "optimizer/vars/34 (32, 32)\n",
      "optimizer/vars/35 (32, 32)\n",
      "optimizer/vars/36 (32, 32)\n",
      "optimizer/vars/37 (32, 32)\n",
      "optimizer/vars/38 (32, 32)\n",
      "optimizer/vars/39 (32, 32)\n",
      "optimizer/vars/4 (448, 1)\n",
      "optimizer/vars/40 (32, 32)\n",
      "optimizer/vars/41 (32, 32)\n",
      "optimizer/vars/5 (448, 1)\n",
      "optimizer/vars/6 (224, 32)\n",
      "optimizer/vars/7 (224, 32)\n",
      "optimizer/vars/8 (32,)\n",
      "optimizer/vars/9 (32,)\n"
     ]
    }
   ],
   "source": [
    "model_path = './reference'\n",
    "with h5py.File(f'{model_path}/autoIntMLP_models_weights.weights.h5', 'r') as f:\n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(name, obj.shape)\n",
    "    f.visititems(print_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
